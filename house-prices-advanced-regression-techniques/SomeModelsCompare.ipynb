{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Hao Jiang\n",
    "\n",
    "This note book will use previous notebook that clean the data. \n",
    "## Goal for this notebook\n",
    " * Try to compare several model with further wrangled data\n",
    "  * LASSO\n",
    "  * Ridge\n",
    "  * RandomForest\n",
    "  * Tuned XGBoost\n",
    " * Generate a output file for submission\n",
    "\n",
    "\n",
    "Some basic ideas are from this kernal:  \n",
    "https://www.kaggle.com/apapiu/house-prices-advanced-regression-techniques/regularized-linear-models    \n",
    "Xgb parameter tuning is based on this page:  \n",
    "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "This part is based on a previous notebook. Many codes are commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import sklearn\n",
    "\n",
    "from scipy.stats import skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine the data and list the names of the columns\n",
    "all_data = pd.concat((train.iloc[:,1:-1], test.iloc[:,1:-1]))\n",
    "# check datasize\n",
    "# all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check na value count\n",
    "# nullCnt = pd.DataFrame({'nullNums' : all_data.isnull().sum()})\n",
    "# nullCnt['DataType'] = all_data[nullCnt.index].dtypes\n",
    "# print nullCnt[nullCnt['nullNums'] > 0].sort_values(by = 'DataType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert all object to categorical data\n",
    "objColumns = all_data.dtypes[all_data.dtypes == 'object'].index\n",
    "\n",
    "# There are several columns of categorical data represented as numeric. Convert them to categorical ones\n",
    "for name in objColumns | pd.Index(['MSSubClass']):\n",
    "    all_data[name] = all_data[name].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert all skewed numerical data and salePrice with log(1+p)\n",
    "target = np.log1p(train['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  Try to convert skewed data, first check the columns\n",
    "numColumns = all_data.dtypes[all_data.dtypes != \"category\"].index\n",
    "skewed_feats = all_data[numColumns].apply(lambda x: skew(x.dropna()))\n",
    "skewed_feats = skewed_feats[skewed_feats > 0.75].index\n",
    "\n",
    "# print skewed_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The skewed features seems find. Lets convert them\n",
    "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at null data.For numerical data, GarageYrBlt, MasVnrArea and LotFrontage seems to have too many nulls.\n",
    "Let's see if we could guess the right value from other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GarageYrBlt\n",
    "garageFeats = pd.Index([name for name in all_data.columns if u'Garage' in name])\n",
    "# all_data.loc[all_data['GarageYrBlt'].isnull(), garageFeats].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that when there's no garage, YrBlt would be null and area data 0. Let's fill the null data for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n",
    "    all_data.loc[all_data['GarageType'].isnull(),[name]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now lets handle LotFrontage\n",
    "lotFeats = [name for name in all_data.columns if u'Lot' in name]\n",
    "# all_data.loc[all_data['LotFrontage'].isnull(), lotFeats].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the none null lot-related data and see if there's any correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n",
    "# facet = sns.FacetGrid(all_data.loc[all_data['LotFrontage'].notnull(), lotFeats], col=\"LotConfig\",  row=\"LotShape\")\n",
    "# facet.map(matplotlib.pyplot.scatter, \"LotArea\", \"LotFrontage\", edgecolor=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LotShape and LotConfig distribution for null value is as followed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pd.pivot_table(all_data.loc[all_data['LotFrontage'].isnull(), lotFeats], values=['LotArea'], index=['LotShape'], columns=['LotConfig'], aggfunc=np.count_nonzero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, null data distribution is similar to none-null ones. And LotFrontage seems to be somewhat linear correlated to LotArea. So let's just predict null LotFrontage with LotArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(all_data.loc[all_data['LotFrontage'].notnull(), ['LotArea']], all_data.loc[all_data['LotFrontage'].notnull(), ['LotFrontage']])\n",
    "all_data.loc[all_data['LotFrontage'].isnull(), ['LotFrontage']] = reg.predict(all_data.loc[all_data['LotFrontage'].isnull(), ['LotArea']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For MasVnrArea it seems that when there is none Masonry veneer the area data apreas null\n",
    "all_data.loc[all_data['MasVnrType'].isnull(), 'MasVnrArea'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There're several pairs of feature that should be merged when one-hot encoded\n",
    "\n",
    "# Deal with Exterior\n",
    "for name in all_data[\"Exterior1st\"].unique().dropna():\n",
    "    all_data[name] = 1 * ((all_data[\"Exterior1st\"] == name) | (all_data[\"Exterior2nd\"] == name))\n",
    "\n",
    "# Deal with Condition\n",
    "for name in all_data[\"Condition1\"].unique().dropna():\n",
    "    all_data[name] = 1 * ((all_data[\"Condition1\"] == name) | (all_data[\"Condition2\"] == name))\n",
    "\n",
    "# Deal with BsmtFinType\n",
    "for name in all_data[\"BsmtFinType1\"].unique().dropna():\n",
    "    all_data[name] = all_data[\"BsmtFinSF1\"] * (all_data[\"BsmtFinType1\"] == name) +  all_data[\"BsmtFinSF2\"] * (all_data[\"BsmtFinType2\"] == name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = all_data.drop(['Exterior1st','Exterior2nd'], axis=1)\n",
    "all_data = all_data.drop(['Condition1','Condition2'], axis=1)\n",
    "all_data = all_data.drop(['BsmtFinType1','BsmtFinType2','BsmtFinSF1','BsmtFinSF2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# one-hot encode category data\n",
    "all_data = pd.get_dummies(all_data)\n",
    "# print all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filling NA's with the mean of the column:\n",
    "all_data = all_data.fillna(all_data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = all_data[:train.shape[0]]\n",
    "X_test  = all_data[train.shape[0]:]\n",
    "y = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Regularized model\n",
    "First we are going to try the l_1(Lasso) and l_2(Ridge) regularized models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn import model_selection, metrics   #Additional scklearn functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(-0.019403690080359526, 0.0045651863797396006, {'alpha': 0.05}),\n",
       "  (-0.018997590938637503, 0.0045209979162896558, {'alpha': 0.1}),\n",
       "  (-0.018201662550210281, 0.0044215159348300406, {'alpha': 0.3}),\n",
       "  (-0.017283492766937514, 0.0042482952611808972, {'alpha': 1}),\n",
       "  (-0.016604091008640132, 0.004037436625102293, {'alpha': 3}),\n",
       "  (-0.016398095867678372, 0.003947864658004189, {'alpha': 5}),\n",
       "  (-0.016287694628734618, 0.0038656994315190989, {'alpha': 10}),\n",
       "  (-0.016349431047858465, 0.0038419995052992144, {'alpha': 15}),\n",
       "  (-0.016758068137764533, 0.0038348818838479597, {'alpha': 30}),\n",
       "  (-0.017350107152205356, 0.0038455046696005278, {'alpha': 50}),\n",
       "  (-0.018002393735585841, 0.0038571532290687998, {'alpha': 75})],\n",
       " {'alpha': 10},\n",
       " 0.12762325269610791)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search or the best result for ridge\n",
    "\n",
    "ridge_param_test = {\n",
    " 'alpha':[0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\n",
    "}\n",
    "\n",
    "ridge_search = GridSearchCV(\n",
    "    estimator = Ridge(),\n",
    "    param_grid = ridge_param_test, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=1,\n",
    "    iid=False, \n",
    "    cv=5\n",
    ")\n",
    "ridge_search.fit(X_train,y)\n",
    "(zip(np.sqrt(-ridge_search.cv_results_['mean_test_score']),ridge_search.cv_results_['params']) ,\n",
    " ridge_search.best_params_, np.sqrt(- ridge_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the rmse for Ridge model is 0.1276. Let's take a look at lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(-0.016232085395533623, 0.0041950625914472361, {'alpha': 0.0001}),\n",
       "  (-0.015329483469259216, 0.003860119514526323, {'alpha': 0.0003}),\n",
       "  (-0.015359527850116322, 0.0038536205468602128, {'alpha': 0.0005}),\n",
       "  (-0.016018442182396835, 0.0040111648600369559, {'alpha': 0.001}),\n",
       "  (-0.020410064136416516, 0.0043397151441104563, {'alpha': 0.005}),\n",
       "  (-0.022661811742487346, 0.004138215280421494, {'alpha': 0.01}),\n",
       "  (-0.042423818298393146, 0.0041580234492563754, {'alpha': 0.05})],\n",
       " {'alpha': 0.0003},\n",
       " 0.12381229126891731)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_param_test = {\n",
    " 'alpha':[0.0001,0.0003,0.0005,0.001,0.005,0.01,0.05]\n",
    "}\n",
    "\n",
    "lasso_search = GridSearchCV(\n",
    "    estimator = Lasso(),\n",
    "    param_grid = lasso_param_test, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=1,\n",
    "    iid=False, \n",
    "    cv=5\n",
    ")\n",
    "lasso_search.fit(X_train,y)\n",
    "(zip(np.sqrt(-lasso_search.cv_results_['mean_test_score']),lasso_search.cv_results_['params'])\n",
    " ,lasso_search.best_params_, np.sqrt(- lasso_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, LASSO seems much better. Now let's see how random forest would perform.\n",
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(-0.021021433861511524,\n",
       "   0.0028575160310576895,\n",
       "   {'max_depth': 14, 'n_estimators': 60}),\n",
       "  (-0.020923134516315004,\n",
       "   0.0028850707389436246,\n",
       "   {'max_depth': 14, 'n_estimators': 70}),\n",
       "  (-0.021022122259037608,\n",
       "   0.0029394006673623919,\n",
       "   {'max_depth': 16, 'n_estimators': 60}),\n",
       "  (-0.020920925338026124,\n",
       "   0.0028251520278058649,\n",
       "   {'max_depth': 16, 'n_estimators': 70})],\n",
       " {'max_depth': 16, 'n_estimators': 70},\n",
       " 0.14464067663705851)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result seems not at all promising, I wonder why is that.   \n",
    "Maybe we could take a deeper look here later.\n",
    "\n",
    "Now let's try to tune a XGBoost model.\n",
    "### XGBoost\n",
    "Same as randomforest model. For each cell here. I have already had some trials before. So please ignore the range I chose here.  \n",
    "The meaning and function for each parameter should be easy to be found at xgb package's git repository:  \n",
    "https://github.com/dmlc/xgboost/blob/master/doc/parameter.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label = y)\n",
    "dtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with tuning tree-based parameters. They are(note that the names here are for sklearn wrapped models, which might be different from the original xgb package): \n",
    " * max_depth\n",
    " * min_child_weight\n",
    " * gamma\n",
    " * subsample\n",
    " * colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(-0.016358093657269406,\n",
       "   0.0026056196331743471,\n",
       "   {'max_depth': 2, 'min_child_weight': 1}),\n",
       "  (-0.016976392433026587,\n",
       "   0.0029711517263745577,\n",
       "   {'max_depth': 2, 'min_child_weight': 2}),\n",
       "  (-0.01727043983643526,\n",
       "   0.0028854543150239041,\n",
       "   {'max_depth': 2, 'min_child_weight': 3}),\n",
       "  (-0.015763988957959822,\n",
       "   0.0028762329835073132,\n",
       "   {'max_depth': 4, 'min_child_weight': 1}),\n",
       "  (-0.015875440446897059,\n",
       "   0.0026706740963414947,\n",
       "   {'max_depth': 4, 'min_child_weight': 2}),\n",
       "  (-0.015935892537781202,\n",
       "   0.0025377571594565156,\n",
       "   {'max_depth': 4, 'min_child_weight': 3})],\n",
       " {'max_depth': 4, 'min_child_weight': 1},\n",
       " 0.1255547249527465)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'max_depth': 4, 'min_child_weight': 1\n",
    "xgb_param_test1 = {\n",
    "    'max_depth':range(2,5,2),\n",
    "    'min_child_weight':range(1,4,1)\n",
    "} \n",
    "xgb_search = GridSearchCV(\n",
    "    estimator = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'reg:linear',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=100\n",
    "    ),\n",
    "    param_grid = xgb_param_test1,\n",
    "    scoring = 'neg_mean_squared_error',\n",
    "    n_jobs = 1,\n",
    "    iid = False,\n",
    "    cv = 5\n",
    ")\n",
    "xgb_search.fit(X_train, y)\n",
    "zip(xgb_search.cv_results_['mean_test_score'], xgb_search.cv_results_['std_test_score'],xgb_search.cv_results_['params']), xgb_search.best_params_,np.sqrt(- xgb_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(-0.015723654975799996, 0.0028170969022582302, {'gamma': 0.004}),\n",
       "  (-0.015733810439397058, 0.0028765401484699394, {'gamma': 0.006}),\n",
       "  (-0.01561262910250231, 0.0027199079760701966, {'gamma': 0.008}),\n",
       "  (-0.015539699371079829, 0.0025360010507298825, {'gamma': 0.01}),\n",
       "  (-0.015728548923363651, 0.002697803166487496, {'gamma': 0.012}),\n",
       "  (-0.015577481542177769, 0.0025692291790647267, {'gamma': 0.014}),\n",
       "  (-0.015707458824718388, 0.0028117312733345104, {'gamma': 0.016})],\n",
       " {'gamma': 0.01},\n",
       " 0.12465833053221846)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.01 seems to be a nice choice for gamma\n",
    "xgb_param_test2 = {\n",
    "    'gamma':[0.01 + 0.002 * x for x in range(-3, 4)]\n",
    "}\n",
    "\n",
    "xgb_search = GridSearchCV(\n",
    "    estimator = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=4,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'reg:linear',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=100\n",
    "    ),\n",
    "    param_grid = xgb_param_test2,\n",
    "    scoring = 'neg_mean_squared_error',\n",
    "    n_jobs = 1,\n",
    "    iid = False,\n",
    "    cv = 5\n",
    ")\n",
    "xgb_search.fit(X_train, y)\n",
    "zip(xgb_search.cv_results_['mean_test_score'], \n",
    "    xgb_search.cv_results_['std_test_score'],\n",
    "    xgb_search.cv_results_['params']\n",
    "), xgb_search.best_params_,np.sqrt(- xgb_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(-0.015892925412810902,\n",
       "   0.0021816744952015887,\n",
       "   {'colsample_bytree': 0.7, 'subsample': 0.4}),\n",
       "  (-0.016265484869282178,\n",
       "   0.0026606678651860676,\n",
       "   {'colsample_bytree': 0.7, 'subsample': 0.45}),\n",
       "  (-0.015601888575414502,\n",
       "   0.0025933323132011645,\n",
       "   {'colsample_bytree': 0.7, 'subsample': 0.5}),\n",
       "  (-0.016057067477646438,\n",
       "   0.0032515708890794629,\n",
       "   {'colsample_bytree': 0.7, 'subsample': 0.55}),\n",
       "  (-0.015709849992268321,\n",
       "   0.0024705712882403339,\n",
       "   {'colsample_bytree': 0.7, 'subsample': 0.6}),\n",
       "  (-0.01630082071065387,\n",
       "   0.0022948798775772909,\n",
       "   {'colsample_bytree': 0.7, 'subsample': 0.65}),\n",
       "  (-0.016027046687784586,\n",
       "   0.0025468515714845374,\n",
       "   {'colsample_bytree': 0.74, 'subsample': 0.4}),\n",
       "  (-0.015800074704189086,\n",
       "   0.0025729198150533906,\n",
       "   {'colsample_bytree': 0.74, 'subsample': 0.45}),\n",
       "  (-0.015939326464065106,\n",
       "   0.0029490277547354307,\n",
       "   {'colsample_bytree': 0.74, 'subsample': 0.5}),\n",
       "  (-0.016340593828188841,\n",
       "   0.003136043454779594,\n",
       "   {'colsample_bytree': 0.74, 'subsample': 0.55}),\n",
       "  (-0.0160475781208619,\n",
       "   0.0024755394786518522,\n",
       "   {'colsample_bytree': 0.74, 'subsample': 0.6}),\n",
       "  (-0.015717921287941077,\n",
       "   0.0025211874617905401,\n",
       "   {'colsample_bytree': 0.74, 'subsample': 0.65}),\n",
       "  (-0.016019135104343839,\n",
       "   0.0020585592477084182,\n",
       "   {'colsample_bytree': 0.78, 'subsample': 0.4}),\n",
       "  (-0.015871318411866852,\n",
       "   0.002749784216335619,\n",
       "   {'colsample_bytree': 0.78, 'subsample': 0.45}),\n",
       "  (-0.015498643263660475,\n",
       "   0.0025850107933774857,\n",
       "   {'colsample_bytree': 0.78, 'subsample': 0.5}),\n",
       "  (-0.016603334163578499,\n",
       "   0.0027505010631532147,\n",
       "   {'colsample_bytree': 0.78, 'subsample': 0.55}),\n",
       "  (-0.015642898648755843,\n",
       "   0.0026084141970026966,\n",
       "   {'colsample_bytree': 0.78, 'subsample': 0.6}),\n",
       "  (-0.015593381267043516,\n",
       "   0.0022735968556345724,\n",
       "   {'colsample_bytree': 0.78, 'subsample': 0.65}),\n",
       "  (-0.0158583318380301,\n",
       "   0.0029521508867542479,\n",
       "   {'colsample_bytree': 0.82, 'subsample': 0.4}),\n",
       "  (-0.015543243514707209,\n",
       "   0.0022796761258530051,\n",
       "   {'colsample_bytree': 0.82, 'subsample': 0.45}),\n",
       "  (-0.016213268179505729,\n",
       "   0.0025331456262558199,\n",
       "   {'colsample_bytree': 0.82, 'subsample': 0.5}),\n",
       "  (-0.016272662072316701,\n",
       "   0.0021248201882330311,\n",
       "   {'colsample_bytree': 0.82, 'subsample': 0.55}),\n",
       "  (-0.015666688469723158,\n",
       "   0.0026633098095517472,\n",
       "   {'colsample_bytree': 0.82, 'subsample': 0.6}),\n",
       "  (-0.016172960079390238,\n",
       "   0.0022041201372303377,\n",
       "   {'colsample_bytree': 0.82, 'subsample': 0.65}),\n",
       "  (-0.016036750188403191,\n",
       "   0.0020544668118683304,\n",
       "   {'colsample_bytree': 0.86, 'subsample': 0.4}),\n",
       "  (-0.016254722880688476,\n",
       "   0.0024299359739695546,\n",
       "   {'colsample_bytree': 0.86, 'subsample': 0.45}),\n",
       "  (-0.015912309270215551,\n",
       "   0.0027024267749071883,\n",
       "   {'colsample_bytree': 0.86, 'subsample': 0.5}),\n",
       "  (-0.015391182213380039,\n",
       "   0.0022986648668355883,\n",
       "   {'colsample_bytree': 0.86, 'subsample': 0.55}),\n",
       "  (-0.015145165103262545,\n",
       "   0.0023865618258911174,\n",
       "   {'colsample_bytree': 0.86, 'subsample': 0.6}),\n",
       "  (-0.015475532527886544,\n",
       "   0.0021594375755544797,\n",
       "   {'colsample_bytree': 0.86, 'subsample': 0.65}),\n",
       "  (-0.015869311647128334,\n",
       "   0.0021963587339362037,\n",
       "   {'colsample_bytree': 0.9, 'subsample': 0.4}),\n",
       "  (-0.016254052230768125,\n",
       "   0.0028333077261477827,\n",
       "   {'colsample_bytree': 0.9, 'subsample': 0.45}),\n",
       "  (-0.016222541021215515,\n",
       "   0.0029045110270093403,\n",
       "   {'colsample_bytree': 0.9, 'subsample': 0.5}),\n",
       "  (-0.016434046338204104,\n",
       "   0.0029405938031369112,\n",
       "   {'colsample_bytree': 0.9, 'subsample': 0.55}),\n",
       "  (-0.016150112835937636,\n",
       "   0.0027198419298704556,\n",
       "   {'colsample_bytree': 0.9, 'subsample': 0.6}),\n",
       "  (-0.015689925122841335,\n",
       "   0.0028767546929984672,\n",
       "   {'colsample_bytree': 0.9, 'subsample': 0.65})],\n",
       " {'colsample_bytree': 0.86, 'subsample': 0.6},\n",
       " 0.12306569425823975)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  {'colsample_bytree': 0.86, 'subsample': 0.6},\n",
    "param_test3 = {\n",
    "    'subsample':[i/100.0 for i in range(40,66,5)],\n",
    "    'colsample_bytree':[i/100.0 for i in range(70,91,4)]\n",
    "}\n",
    "\n",
    "xgb_search = GridSearchCV(\n",
    "    estimator = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=4,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.01,\n",
    "        subsample=0.6,\n",
    "        colsample_bytree=0.86,\n",
    "        objective= 'reg:linear',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        seed=100\n",
    "    ),\n",
    "    param_grid = param_test3,\n",
    "    scoring = 'neg_mean_squared_error',\n",
    "    n_jobs = 1,\n",
    "    iid = False,\n",
    "    cv = 5\n",
    ")\n",
    "xgb_search.fit(X_train, y)\n",
    "zip(xgb_search.cv_results_['mean_test_score'],xgb_search.cv_results_['std_test_score'],xgb_search.cv_results_['params']), xgb_search.best_params_,np.sqrt(- xgb_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to train some regulation parameter. Since Lasso works better in former session. Using reg_alpha seems to be a better choice here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(-0.015145166354327499, 0.0023865603070679564, {'reg_alpha': 1e-07}),\n",
       "  (-0.015145173263389275, 0.0023865647056021838, {'reg_alpha': 5e-07}),\n",
       "  (-0.015145176154974297, 0.0023865724076833216, {'reg_alpha': 1e-06}),\n",
       "  (-0.01514517333398129, 0.0023865607389549669, {'reg_alpha': 5e-06}),\n",
       "  (-0.01514517164947366, 0.0023865640014755639, {'reg_alpha': 1e-05})],\n",
       " {'reg_alpha': 1e-07},\n",
       " 0.12306569934115476)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  'reg_alpha': 1e-07. It seems that there's only minor change\n",
    "param_test4 = {\n",
    "    'reg_alpha':[1e-7, 5e-7, 1e-6, 5e-6, 1e-5]\n",
    "}\n",
    "\n",
    "xgb_search = GridSearchCV(\n",
    "    estimator = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=140,\n",
    "        max_depth=4,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.01,\n",
    "        subsample=0.6,\n",
    "        colsample_bytree=0.86,\n",
    "        objective= 'reg:linear',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        reg_alpha = 1e-7,\n",
    "        seed=100\n",
    "    ),\n",
    "    param_grid = param_test4,\n",
    "    scoring = 'neg_mean_squared_error',\n",
    "    n_jobs = 1,\n",
    "    iid = False,\n",
    "    cv = 5\n",
    ")\n",
    "xgb_search.fit(X_train, y)\n",
    "zip(xgb_search.cv_results_['mean_test_score'],xgb_search.cv_results_['std_test_score'],xgb_search.cv_results_['params']), xgb_search.best_params_,np.sqrt(- xgb_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we've done the tuning. Now we could build the final model by reducing learning_rete and increasing number of estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.0003, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = XGBRegressor(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=5000,\n",
    "        max_depth=4,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.01,\n",
    "        subsample=0.6,\n",
    "        colsample_bytree=0.86,\n",
    "        objective= 'reg:linear',\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1,\n",
    "        reg_alpha = 1e-7,\n",
    "        seed=100\n",
    ")\n",
    "xgb_model.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_preds = np.expm1(xgb_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the output for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pure_xgb = pd.DataFrame({\"id\":test.Id, \"SalePrice\":xgb_preds})\n",
    "pure_xgb.to_csv(\"pure_xgb.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
